{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuBkuCBVQ5f3"
      },
      "source": [
        "# **Resume Categorization**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbGi7-LDQ8iE",
        "outputId": "6993ab83-801f-43da-d48a-35fbc3821d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY4ACGecQ5f6"
      },
      "source": [
        "![Resume Cateorization](CreatingaPhysicianCVthatShines.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSKnvSerQ5f6"
      },
      "source": [
        "This project tackles the challenge of **resume categorization** using machine learning and deep learning techniques. Companies often face the daunting task of sifting through numerous resumes for each job opening. This app aims to automate and streamline this process by predicting the job category a given resume belongs to. By using a trained model, the app can quickly suggest the appropriate job category for each resume, saving time and resources for recruiters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55lhuCZSQ5f7"
      },
      "source": [
        "## **Set Environmnt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AjHfMbfsQ5f7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Acquisition:** The project uses the [Resume Dataset](https://www.kaggle.com/datasets/gauravduttakiit/resume-dataset) from Kaggle. This dataset consists of resumes categorized into 25 distinct job fields, providing a solid foundation for training our models."
      ],
      "metadata": {
        "id": "wsDLj-KLT-V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/UpdatedResumeDataSet.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EiRaDy_2UAqh",
        "outputId": "f6b05e9d-9081-4b15-d5ce-0586c9094ea1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Category                                             Resume\n",
              "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
              "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
              "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
              "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
              "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fa798f9-b13b-43ee-8a57-9fe9039b41ce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Resume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Science</td>\n",
              "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Science</td>\n",
              "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Science</td>\n",
              "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data Science</td>\n",
              "      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Data Science</td>\n",
              "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fa798f9-b13b-43ee-8a57-9fe9039b41ce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6fa798f9-b13b-43ee-8a57-9fe9039b41ce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6fa798f9-b13b-43ee-8a57-9fe9039b41ce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-757735ef-04bb-4bbd-b044-8b6a207d337b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-757735ef-04bb-4bbd-b044-8b6a207d337b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-757735ef-04bb-4bbd-b044-8b6a207d337b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 962,\n  \"fields\": [\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"Civil Engineer\",\n          \"DevOps Engineer\",\n          \"Data Science\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Resume\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 166,\n        \"samples\": [\n          \"KEY COMPETENCIES \\u00e2\\u009c\\u00b6Multi - Operations Management\\u00e2\\u009c\\u00b6People Management \\u00e2\\u009c\\u00b6Customer Services - Emails \\u00e2\\u009c\\u00b6 MIS \\u00e2\\u009c\\u00b6Vendor & Client Services Management\\u00e2\\u009c\\u00b6Cross Functional Coordination\\u00e2\\u009c\\u00b6Banking & Financial Services\\u00e2\\u009c\\u00b6 Transaction Monitoring * ATM Operations \\u00e2\\u009c\\u00b6 & Prepaid Card Operations (Pre-Issuance & Post-Issuance) \\u00e2\\u009c\\u00b6 POS Operations * JOB PROFILE & SKILLS: \\u00e2\\u0080\\u00a2 An effective communicator with excellent relationship building & interpersonal skills. Strong analytical, problem solving & organizational abilities. \\u00e2\\u0080\\u00a2 Extensive experience in managing operations with demonstrated leadership qualities & organisational skills during the tenure. \\u00e2\\u0080\\u00a2 Managing customer centric operations & ensuring customer satisfaction by achieving service quality norms. \\u00e2\\u0080\\u00a2 Analyzing of all operational problems, customer complaints and take preventive and corrective actions to resolve the same. \\u00e2\\u0080\\u00a2 Receive and respond to Key customer inquiries in an effective manner and provide relevant and timely information. \\u00e2\\u0080\\u00a2 Deft in steering banking back-end operations, analyzing risks and managing delinquencies with dexterity across applying techniques for maximizing recoveries and minimizing credit losses. \\u00e2\\u0080\\u00a2 Analyzed & identified training needs of the team members and developing, organizing and conducting training programs and manage bottom quartile team to improve their performance. \\u00e2\\u0080\\u00a2 Preparing and maintaining daily MIS reports to evaluate the performance and efficiency of the process relate to various verticals. \\u00e2\\u0080\\u00a2 Measuring the performance of the processes in terms of efficiency and effectiveness matrix and ensuring adherence to SLA. \\u00e2\\u0080\\u00a2 Major Activities Define processes for Field Services were monitored and necessary checks were executed and controlled. Also measured Vendor SLA by analyzing the TAT of vendors & the Client SLA provided to us. \\u00e2\\u0080\\u00a2 As per company procedures, handling & ensuring vendor's payment issues to be sorted out &payments are processed on quarterly basis. \\u00e2\\u0080\\u00a2 Appropriately plan and execute each skill of operations in accordance with the department's policies and procedures. \\u00e2\\u0080\\u00a2 Manage relationships with business team, software development team and other services to achieve project objectives. Different software Worked till now: - a. CTL prime - Axis Bank Credit Cards b. Insight - For POS Machine technical operations for Amex (MID & TID Generation- ATOS (Venture Infotek) c. Ticket Management System - TATA Communications Private Services Ltd (ATM - NOC Operations) d. Branch Portal (Yalamanchili Software Exports Ltd) - Prepaid Cards (SBI Bank & Zaggle Prepaid Oceans Services Ltd) Zaggle Prepaid Ocean Services Pvt Ltd Oct, 2017 to Till Date Designation: Manager - Operations (Payment Industry - Prepaid Cards - INR) Education Details \\r\\n  Commerce Mumbai, Maharashtra Mumbai University\\r\\nOperations Manager \\r\\n\\r\\nService Manager - Operations (Payment Industry - Prepaid Cards - INR & FTC)\\r\\nSkill Details \\r\\nOPERATIONS- Exprience - 73 months\\r\\nSATISFACTION- Exprience - 48 months\\r\\nTRAINING- Exprience - 24 months\\r\\nNOC- Exprience - 23 months\\r\\nPOINT OF SALE- Exprience - 20 monthsCompany Details \\r\\ncompany - Zaggle Prepaid Ocean Services Pvt Ltd\\r\\ndescription - Card Operations\\r\\ncompany - Yalamanchili Software Exports Ltd\\r\\ndescription - 24*7 Operations Pvt Ltd) Dec 2015 to Feb 2017\\r\\n\\r\\nDesignation: Service Manager - Operations (Payment Industry - Prepaid Cards - INR & FTC)\\r\\n\\r\\nKey Contributions: \\u00e2\\u0080\\u00a2 A result-oriented business professional in planning, executing& managing processes, improving efficiency of operations, team building and detailing process information to determine effective result into operations.\\r\\n\\u00e2\\u0080\\u00a2 Ensuring PINs generation (SLA) is maintained and chargeback cases are raised in perfect timeframe.\\r\\n\\u00e2\\u0080\\u00a2 Managing email customer services properly and ensuring the emails are replied properly. Also, ensuring transaction monitoring is properly managed 24/7.\\r\\n\\u00e2\\u0080\\u00a2 Assisting Bankers (SBI & Associated Banks) for their BCP plans by getting executed in the system with the help of DR-PR plans & vice versa or any other business requirements.\\r\\n\\u00e2\\u0080\\u00a2 Expertise in maintaining highest level of quality in operations; ensuring adherence to all the quality parameters and procedures as per the stringent norms.\\r\\n\\u00e2\\u0080\\u00a2 Lead, manage and supervise the execution of external audit engagements and responsible for presenting the findings & developing a quality reports to the senior Management and Clients.\\r\\n\\u00e2\\u0080\\u00a2 Coach/mentor (20) team members to perform at a higher level by giving opportunities, providing timely continuous feedback and working with staff to improve their communication, time management, decision making, organization, and analytical skills.\\r\\n\\u00e2\\u0080\\u00a2 Providing the solutions and services to the client in their own premises with aforesaid count of team members.\\r\\n\\u00e2\\u0080\\u00a2 Also ensuring end to end process of PR & DR as per client requirements (PR- DR & DR -PR) by interacting with internal & external stakeholders.\\r\\n\\u00e2\\u0080\\u00a2 Determining process gaps and designing & conducting training programs to enhance operational efficiency and retain talent by providing optimum opportunities for personal and professional growth.\\r\\ncompany - Credit Cards\\r\\ndescription - Ensured highest standard of customer satisfaction and quality service; developing new policies and procedures to improve based on customer feedback and resolving customer queries via correspondence, inbound calls & email channels with the strength of (12-16) Team members.\\r\\ncompany - AGS Transact Technologies Limited\\r\\ndescription - Key Contributions: Lead - SPOC to Banks\\r\\ncompany - TATA Communications Payment Solutions Ltd\\r\\ndescription - To make ATMs operational within TAT by analyzing the issue is technical or non-technical and also by interacting with internal & external stakeholders.\\r\\ncompany - Vertex Customer Solutions India Private Ltd\\r\\ndescription - Key Contributions: \\u00e2\\u0080\\u00a2 Build positive working relationship with all team members and clients by keeping Management informed   of KYC document collection & con-current audit progress, responding timely to Management inquiries, understanding the business and conducting self professionally.\\r\\ncompany - Financial Inclusion Network & Operations Limited\\r\\ndescription - Key Contributions: POS-Operations \\u00e2\\u0080\\u00a2 Cascading the adherence of process is strictly followed by team members & training them to reduce the downtime.\\r\\n\\u00e2\\u0080\\u00a2 Managing Stock of EDC Terminals \\u00e2\\u0080\\u00a2 Managing Deployments of terminals through Multiple teams \\u00e2\\u0080\\u00a2 Would have worked with multiple terminal make & model \\u00e2\\u0080\\u00a2 Managing Inward, Outward & QC of applications installed in the POS machines.\\r\\ncompany - Venture Infotek Private Ltd\\r\\ndescription - Key Contributions: POS-Operations\\r\\ncompany - Axis Bank Ltd - Customer Services\\r\\ndescription - Aug 2006 to Oct 2009 (Ma-Foi&I- smart)\\r\\n\\r\\nDesignation: Team Leader/Executive - Emails, Phone Banking & Correspondence Unit (Snail Mails)\",\n          \"Skill Set: Hadoop, Map Reduce, HDFS, Hive, Sqoop, java. Duration: 2016 to 2017. Role: Hadoop Developer Rplus offers an quick, simple and powerful cloud based Solution, Demand Sense to accurately predict demand for your product in all your markets which Combines Enterprise and External Data to predict demand more accurately through Uses Social Conversation and Sentiments to derive demand and Identifies significant drivers of sale out of hordes of factors that Selects the best suited model out of multiple forecasting models for each product. Responsibilities: \\u00e2\\u0080\\u00a2 Involved in deploying the product for customers, gathering requirements and algorithm optimization at backend of the product. \\u00e2\\u0080\\u00a2 Load and transform Large Datasets of structured semi structured. \\u00e2\\u0080\\u00a2 Responsible to manage data coming from different sources and application \\u00e2\\u0080\\u00a2 Supported Map Reduce Programs those are running on the cluster \\u00e2\\u0080\\u00a2 Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.Education Details \\r\\n\\r\\nHadoop Developer \\r\\n\\r\\nHadoop Developer - Braindatawire\\r\\nSkill Details \\r\\nAPACHE HADOOP HDFS- Exprience - 49 months\\r\\nAPACHE HADOOP SQOOP- Exprience - 49 months\\r\\nHadoop- Exprience - 49 months\\r\\nHADOOP- Exprience - 49 months\\r\\nHADOOP DISTRIBUTED FILE SYSTEM- Exprience - 49 monthsCompany Details \\r\\ncompany - Braindatawire\\r\\ndescription - Technical Skills:\\r\\n\\u00e2\\u0080\\u00a2   Programming: Core Java, Map Reduce, Scala\\r\\n\\u00e2\\u0080\\u00a2   Hadoop Tools: HDFS, Spark, Map Reduce, Sqoop, Hive, Hbase\\r\\n\\u00e2\\u0080\\u00a2   Database: MySQL, Oracle\\r\\n\\u00e2\\u0080\\u00a2   Scripting: Shell Scripting\\r\\n\\u00e2\\u0080\\u00a2   IDE: Eclipse\\r\\n\\u00e2\\u0080\\u00a2   Operating Systems: Linux (CentOS), Windows\\r\\n\\u00e2\\u0080\\u00a2   Source Control: Git (Github)\",\n          \"IT Skills: Area Exposure Modeling Tool: Bizagi, MS Visio Prototyping Tool: Indigo Studio. Documentation: MS Office (MS Word, MS Excel, MS Power Point) Testing Proficiency: Smoke, Sanity, Integration, Functional, Acceptance and UI Methodology implemented: Waterfall, Agile (Scrum) Database: SQL Testing Tool: HPQC Business Exposure Education Details \\r\\n Bachelor Of Computer Engineering Computer Engineering Mumbai, Maharashtra Thadomal Shahani Engineering college\\r\\n Diploma Computer Engineering Ulhasnagar, Maharashtra Institute of Technology\\r\\n Secondary School Certificate  Ulhasnagar, Maharashtra New English High School\\r\\nSenior Business Analyst - RPA \\r\\n\\r\\nSenior Business Analyst - RPA - Hexaware Technologies\\r\\nSkill Details \\r\\nDOCUMENTATION- Exprience - 47 months\\r\\nTESTING- Exprience - 29 months\\r\\nINTEGRATION- Exprience - 25 months\\r\\nINTEGRATOR- Exprience - 25 months\\r\\nPROTOTYPE- Exprience - 13 monthsCompany Details \\r\\ncompany - Hexaware Technologies\\r\\ndescription - Working as a RPA Business Analyst\\r\\ncompany - BBH- Brown Brothers Harriman & Co\\r\\ndescription - is a private bank that provides commercial banking, investment management, brokerage, and trust services to private companies and individuals. It also performs merger advisory, foreign exchange, custody services, commercial banking, and corporate financing services.\\r\\n\\r\\nResponsibilities: \\u00e2\\u0080\\u00a2 Performed Automation Assessment of various Processes and identified processes which can be candidates of RPA.\\r\\n\\u00e2\\u0080\\u00a2 Conducting Assessment that involves an initial Understanding of the Existing System, their technology, processes, Usage of the tools, Feasibility of tool with automation tool along with automation ROI analysis.\\r\\n\\u00e2\\u0080\\u00a2 Preparing the Automation Potential Sheet which describes the steps in the process, the volume and frequency of the transaction, the AHT taken by SME to perform the process and depending on the steps that could be automated, Automation potential and the manual efforts that will be saved are calculated.\\r\\nCalculating the complexity of the Process which is considered for automation and depending on all these factors Number of Bots and Number of Automation tool Licenses are determined.\\r\\n\\u00e2\\u0080\\u00a2 Implementing a Proof of Concept (POC) to Validate Feasibility by executing the selected critical use cases for conducting a POC which will helps to identify financial and operational benefits and provide recommendations regarding the actual need for complete automation.\\r\\n\\u00e2\\u0080\\u00a2 Gathering business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME's) \\u00e2\\u0080\\u00a2 Preparing Business Requirement Document and then converted Business requirements into Functional Requirements Specification.\\r\\n \\u00e2\\u0080\\u00a2 Constructing prototype early toward a design acceptable to the customer and feasible.\\r\\n\\u00e2\\u0080\\u00a2 Assisting in designing test plans, test scenarios and test cases for integration, regression, and user acceptance testing (UAT) to improve the overall quality of the Automation.\\r\\n\\u00e2\\u0080\\u00a2 Participating regularly in Walkthroughs and Review meetings with Project Manager, QA Engineers, and Development team.\\r\\n\\u00e2\\u0080\\u00a2 Regularly interacting with offshore and onshore development teams.\\r\\ncompany - FADV - First Advantage\\r\\ndescription - is a criminal background check company that delivers global solutions ranging from employment screenings to background checks.\\r\\nThe following are the processes which were covered:\\r\\nEmail Process, Research Process, Review Process.\\r\\n\\r\\nResponsibilities: \\u00e2\\u0080\\u00a2 Requirement Gathering through conducting Interviews & Brainstorming sessions with stakeholders \\u00e2\\u0080\\u00a2 To develop decision models and execute those rules as per the use case specifications.\\r\\n\\u00e2\\u0080\\u00a2 To Test/validate the decision models against document test data.\\r\\n\\u00e2\\u0080\\u00a2 To maintain and enhance the decision models for changes in regulations as per use case specifications.\\r\\n\\u00e2\\u0080\\u00a2 Responsible for performing the business research that will make a business growth.\\r\\n\\u00e2\\u0080\\u00a2 Developing a clear understanding of existing business functions and processes.\\r\\n\\u00e2\\u0080\\u00a2 Effectively communicate with the onsite clients for the queries, suggestions, and update.\\r\\n\\u00e2\\u0080\\u00a2 Giving suggestions to enhance the current processes.\\r\\n\\u00e2\\u0080\\u00a2 Identifying areas for process improvement.\\r\\n\\u00e2\\u0080\\u00a2 Flagging up potential problems at an early stage.\\r\\n\\u00e2\\u0080\\u00a2 Preparing PowerPoint presentations and documents for business meetings.\\r\\n\\u00e2\\u0080\\u00a2 Using any information gathered to write up detailed reports.\\r\\n\\u00e2\\u0080\\u00a2 Highlighting risks and issues that could impact project delivery.\\r\\n\\u00e2\\u0080\\u00a2 Able to work accurately.\\r\\n\\u00e2\\u0080\\u00a2 To develop and maintain documentation for internal team training and client end user operations.\\r\\n\\u00e2\\u0080\\u00a2 To work efficiently with team members and across teams.\\r\\n\\u00e2\\u0080\\u00a2 To mentor and train junior team members.\\r\\ncompany - Clinical Testing, Lab Work and Diagnostic Testing\\r\\ndescription - IQVIA provides services to its customers this includes: Clinical Testing, Lab Work and Diagnostic Testing under clinical trial. These customers need to pay to IQVIA and aging details and invoices are generated for the same.\\r\\nThe following are the processes which were covered:\\r\\n\\r\\nTracking Payments, Automated Real Time Metrics Reporting (Dashboard), Past Due Notifications, AR Statements, Credit/Rebill.\\r\\nResponsibilities: \\u00e2\\u0080\\u00a2 Conducting meetings with clients and key stakeholders to gather requirements, analyze, finalize and have formal sign-offs from approvers Gather and perform analysis of the business requirements \\u00e2\\u0080\\u00a2 Translating the business requirements into the Business Requirement Document [BRD], Functional Requirement Document [FRD].\\r\\n\\u00e2\\u0080\\u00a2 Facilitating meetings with the appropriate subject matter experts in both business and technology teams \\u00e2\\u0080\\u00a2 Coordinating with business user community for the execution of user acceptance test as well as tracking issues \\u00e2\\u0080\\u00a2 Working, collaborating and coordinating with Offshore and Onsite team members to fulfill the BA responsibilities from project initiation to Post-Implementation \\u00e2\\u0080\\u00a2 Reviewing the test scripts with business users as well as technology team. Execute test scripts with expected results for the System Integration Test (SIT) and User Acceptance Test (UAT) \\u00e2\\u0080\\u00a2 Coordinating and conducting the Production Acceptance Testing (PAT) with the business users \\u00e2\\u0080\\u00a2 Creating flow diagrams, structure charts, and other types of system or process representations \\u00e2\\u0080\\u00a2 Managing changes to requirements and baseline through a change control process \\u00e2\\u0080\\u00a2 Utilizing standard methods, design and testing tools throughout project development life cycle \\u00e2\\u0080\\u00a2 Work closely with the operational functional teams, operations management, and personnel, and various technology teams to facilitate a shared understanding of requirements and priorities across all areas\\r\\ncompany - Eduavenir IT Solution\\r\\ndescription - Project: M.B.M.S\\r\\n\\r\\nM.B.M.S. - is an Inventory management application that allows user to manage inventory details of different warehouses, having different products located at various locations and help extract what goods have been procured, sold or returned by customers. It generates automated invoicesalong withcustomized reports. It also managescustomer complaint and resolution system implementation along with automated MIS on monthly basis.Sales and forecastingis also developed on MIS System and the streamlining of process of warehousing and dispatch along with online proof of delivery management system (POD documentation) is generated.\\r\\n\\r\\nResponsibilities: \\u00e2\\u0080\\u00a2 Participate in requirement gathering discussion with client to understand the flow of business processes \\u00e2\\u0080\\u00a2 Analyze the requirements and determine the core processes, develop Process Documentation and ensure to stay up-to-date in conjunction with on-going changes \\u00e2\\u0080\\u00a2 Participate in process flow analysis and preparing BRD, SRS.\\r\\n\\u00e2\\u0080\\u00a2 Coordinating with developers, designers & operations teams for various nuances of the project, communicate the stakeholder requirements from requirement /enhancement to implementation and finally deliver the same within estimated timeframe.\\r\\n\\u00e2\\u0080\\u00a2 Support UAT by reviewing test cases, manage version control of documents, software builds.\\r\\n\\u00e2\\u0080\\u00a2 Coordinate with the stakeholders for UAT sign off and coordinate internally for production movement till Golive stage of the application.\\r\\n\\u00e2\\u0080\\u00a2 Provide demo and training to internal and end user using PowerPoint presentation.\\r\\n\\u00e2\\u0080\\u00a2 Resolving project functional &technical issues during UAT.\\r\\n\\u00e2\\u0080\\u00a2 Prioritizing the Production bugs and resolving the same within the estimated timeframe.\\r\\n\\u00e2\\u0080\\u00a2 Preparing Project Status Report and Production Bugs Status to all the stakeholders.\\r\\n\\u00e2\\u0080\\u00a2 Promoting and Networking for online trading platform.\\r\\n\\u00e2\\u0080\\u00a2 Designing query sheet for obtaining and comparison of quotes from various vendors.\\r\\n\\u00e2\\u0080\\u00a2 Development of product codes / material codes for inventory management (Master Data Management)\\r\\ncompany - CAPGEMINI Head Office\\r\\ndescription - Type: Mobile and Device Testing.       Duration: January 2014 - August 2014\\r\\n\\r\\nFollet - An application which takes an electronic request from the user for the books he requires from a particular follet store. This detailed information about books that will include the name of the book, its price, the date of the transaction and the parties involved which will then be sent to follet stores. User then create request for one or more books for a given date. This request is then processed further and user gets a mail of the date when he will be provided with that book.\\r\\n\\r\\nResponsibilities: \\u00e2\\u0080\\u00a2 Understanding the needs and business requirements.\\r\\n\\u00e2\\u0080\\u00a2 Preparing BRD, SRS by eliciting all the requirements from the client and SMEs \\u00e2\\u0080\\u00a2 Understanding the dependency of the modules in the system \\u00e2\\u0080\\u00a2 Preparation of test plan for Unit level and Integration level.\\r\\n\\u00e2\\u0080\\u00a2 Preparation and execution of test cases.\\r\\n\\u00e2\\u0080\\u00a2 Defect tracking, Issue Resolution, Risk Monitoring, Status Tracking, Reporting and Follow-up.\\r\\n\\u00e2\\u0080\\u00a2 Preparation of Test Completion report.\\r\\ncompany - CAPGEMINI Head Office\\r\\ndescription - \\r\\ncompany - CAPGEMINI Head Office\\r\\ndescription - Humana is a health care insurance project of U.S. which deals with supplying various medicines to citizens as per the doctor's reference and patient's insurance policy. This application keeps track of all the medicines user has consumed in the past and generates a patient history. A citizen is given a drug only after the doctor's reference so the doctor's information is also linked with the patient's history.\\r\\n\\r\\nResponsibilities: \\u00e2\\u0080\\u00a2 Understanding the requirements and getting clarifications from client.\\r\\n\\u00e2\\u0080\\u00a2 Involved in writing test cases based on test scenarios and execute them.\\r\\n\\u00e2\\u0080\\u00a2 Ensuring Test Coverage using Requirement Traceability Matrix (RTM) \\u00e2\\u0080\\u00a2 Preparation of Test Completion report.\\r\\ncompany - CAPGEMINI Head Office\\r\\ndescription - Testing Trends WQR (World Quality Report) is an application which allows the users to take a survey on different methods and technologies used for testing. Users can choose to answer any type of questions under three different categories. Users have a facility to search, view and export the data to excel. Also, users get daily and weekly reports through email about the new trends in testing implemented around the globe. Testing Trends WQR app is available on Android and IOS platforms.\\r\\n\\r\\nResponsibilities: \\u00e2\\u0080\\u00a2 Understanding the requirements and getting clarifications from client.\\r\\n\\u00e2\\u0080\\u00a2 Writing test cases based on test scenarios and executed them.\\r\\n\\u00e2\\u0080\\u00a2 Performing different types of testing such as Functional, Integration, System, and UAT.\\r\\n\\u00e2\\u0080\\u00a2 Defect resolution and maintenance of the application.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Displaying the distinct categories of resume**"
      ],
      "metadata": {
        "id": "W4iGeCijUKnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Displaying the distinct categories of resume:\\n\\n \")\n",
        "print (df['Category'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz1fx8OsUOJU",
        "outputId": "01f501ca-888e-4bb7-bbf7-b6514710286a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying the distinct categories of resume:\n",
            "\n",
            " \n",
            "['Data Science' 'HR' 'Advocate' 'Arts' 'Web Designing'\n",
            " 'Mechanical Engineer' 'Sales' 'Health and fitness' 'Civil Engineer'\n",
            " 'Java Developer' 'Business Analyst' 'SAP Developer' 'Automation Testing'\n",
            " 'Electrical Engineering' 'Operations Manager' 'Python Developer'\n",
            " 'DevOps Engineer' 'Network Security Engineer' 'PMO' 'Database' 'Hadoop'\n",
            " 'ETL Developer' 'DotNet Developer' 'Blockchain' 'Testing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Displaying the number of resumes in each category**"
      ],
      "metadata": {
        "id": "K5teceqvUTBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Displaying the distinct categories of resume and the number of records belonging to each category:\\n\\n\")\n",
        "print (df['Category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkuK2InVUV9W",
        "outputId": "02aa1e38-b250-4c3c-f428-e3864bf6b094"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying the distinct categories of resume and the number of records belonging to each category:\n",
            "\n",
            "\n",
            "Category\n",
            "Java Developer               84\n",
            "Testing                      70\n",
            "DevOps Engineer              55\n",
            "Python Developer             48\n",
            "Web Designing                45\n",
            "HR                           44\n",
            "Hadoop                       42\n",
            "Sales                        40\n",
            "Data Science                 40\n",
            "Mechanical Engineer          40\n",
            "ETL Developer                40\n",
            "Blockchain                   40\n",
            "Operations Manager           40\n",
            "Arts                         36\n",
            "Database                     33\n",
            "Health and fitness           30\n",
            "PMO                          30\n",
            "Electrical Engineering       30\n",
            "Business Analyst             28\n",
            "DotNet Developer             28\n",
            "Automation Testing           26\n",
            "Network Security Engineer    25\n",
            "Civil Engineer               24\n",
            "SAP Developer                24\n",
            "Advocate                     20\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check the dataset**"
      ],
      "metadata": {
        "id": "lfjbD9oxUafv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Category'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ffNu6URLUbcC",
        "outputId": "439f2b4a-2756-4cf6-9e7a-e48d0e7cf1b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data Science'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Resume'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "sK8-x4WAUeKw",
        "outputId": "8c6effea-c7c5-4b16-e432-511745933d01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \\r\\n\\r\\nData Science Assurance Associate \\r\\n\\r\\nData Science Assurance Associate - Ernst & Young LLP\\r\\nSkill Details \\r\\nJAVASCRIPT- Exprience - 24 months\\r\\njQuery- Exprience - 24 months\\r\\nPython- Exprience - 24 monthsCompany Details \\r\\ncompany - Ernst & Young LLP\\r\\ndescription - Fraud Investigations and Dispute Services   Assurance\\r\\nTECHNOLOGY ASSISTED REVIEW\\r\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\r\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\\r\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\\r\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\\r\\n\\r\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\r\\n\\r\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\r\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\\r\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\\r\\n* Created customized tableau dashboards for effective reporting and visualizations.\\r\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\\r\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\\r\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\\r\\n\\r\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\\r\\n\\r\\nINFORMATION GOVERNANCE\\r\\nOrganizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\\r\\n* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\\r\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\\r\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\\r\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\r\\n\\r\\nFRAUD ANALYTIC PLATFORM\\r\\nFraud Analytics and investigative platform to review all red flag cases.\\r\\nâ\\x80¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\\r\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\\r\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsnnxF38Q5f_"
      },
      "source": [
        "## **Text preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJf9Dm7KQ5f_"
      },
      "source": [
        "**Text Preprocessing:** Raw resume text is often messy. We apply preprocessing techniques to clean the resume data, including:\n",
        "- Removing URLs, RTs, hashtags, and mentions\n",
        "- Eliminating special characters and non-ASCII characters\n",
        "- Collapsing extra whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7BSiK636Q5f_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def cleanResume(txt):\n",
        "    cleanText = re.sub('http\\S+\\s', ' ', txt) # This line removes any URLs from the text\n",
        "    cleanText = re.sub('RT|cc', ' ', cleanText) # This line removes any RTs or cc from the text\n",
        "    cleanText = re.sub('#\\S+\\s', ' ', cleanText) # This line removes any hashtags from the text\n",
        "    cleanText = re.sub('@\\S+', '  ', cleanText) # This line removes any @ from the text\n",
        "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText) # This line removes any punctuations from the text\n",
        "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) # This line removes any non-ASCII characters from the text\n",
        "    cleanText = re.sub('\\s+', ' ', cleanText) # This line removes any extra whitespaces from the text\n",
        "    return cleanText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Resume'] = df['Resume'].apply(lambda x: cleanResume(x))"
      ],
      "metadata": {
        "id": "goU65X_TUkf3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check cleaned text**"
      ],
      "metadata": {
        "id": "BPF3alOaUntj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Resume'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "IrW57I7nUomj",
        "outputId": "34f1cd11-7ec6-497f-a26c-c7f0303bfa7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Skills Programming Languages Python pandas numpy scipy scikit learn matplotlib Sql Java JavaScript JQuery Machine learning Regression SVM Na ve Bayes KNN Random Forest Decision Trees Boosting techniques Cluster Analysis Word Embedding Sentiment Analysis Natural Language processing Dimensionality reduction Topic Modelling LDA NMF PCA Neural Nets Database Visualizations Mysql SqlServer Cassandra Hbase ElasticSearch D3 js DC js Plotly kibana matplotlib ggplot Tableau Others Regular Expression HTML CSS Angular 6 Logstash Kafka Python Flask Git Docker computer vision Open CV and understanding of Deep learning Education Details Data Science Assurance Associate Data Science Assurance Associate Ernst Young LLP Skill Details JAVASCRIPT Exprience 24 months jQuery Exprience 24 months Python Exprience 24 monthsCompany Details company Ernst Young LLP description Fraud Investigations and Dispute Services Assurance TECHNOLOGY ASSISTED REVIEW TAR Technology Assisted Review assists in a elerating the review process and run analytics and generate reports Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain this tool implements predictive coding and topic modelling by automating reviews resulting in reduced labor costs and time spent during the lawyers review Understand the end to end flow of the solution doing research and development for classification models predictive analysis and mining of the information present in text data Worked on analyzing the outputs and precision monitoring for the entire tool TAR assists in predictive coding topic modelling from the evidence by following EY standards Developed the classifier models in order to identify red flags and fraud related issues Tools Technologies Python scikit learn tfidf word2vec doc2vec cosine similarity Na ve Bayes LDA NMF for topic modelling Vader and text blob for sentiment analysis Matplot lib Tableau dashboard for reporting MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS USA CLIENTS TEXT ANALYTICS MOTOR VEHICLE CUSTOMER REVIEW DATA Received customer feedback survey data for past one year Performed sentiment Positive Negative Neutral and time series analysis on customer comments across all 4 categories Created heat map of terms by survey category based on frequency of words Extracted Positive and Negative words across all the Survey categories and plotted Word cloud Created customized tableau dashboards for effective reporting and visualizations CHATBOT Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation reservation options and so on This chat bot serves entire product related questions Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant recommended questions Tools Technologies Python Natural language processing NLTK spacy topic modelling Sentiment analysis Word Embedding scikit learn JavaScript JQuery SqlServer INFORMATION GOVERNANCE Organizations to make informed decisions about all of the information they store The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk Scan data from multiple sources of formats and parse different file formats extract Meta data information push results for indexing elastic search and created customized interactive dashboards using kibana Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant Outdated or Trivial Preforming full text search analysis on elastic search with predefined methods which can tag as PII personally identifiable information social security numbers addresses names etc which frequently targeted during cyber attacks Tools Technologies Python Flask Elastic Search Kibana FRAUD ANALYTIC PLATFORM Fraud Analytics and investigative platform to review all red flag cases FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems It can be used by clients to interrogate their A ounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics Tools Technologies HTML JavaScript SqlServer JQuery CSS Bootstrap Node js D3 js DC js'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Preprocessing**\n"
      ],
      "metadata": {
        "id": "pyYGeeltUr6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Data Preparation:**\n",
        "- **Category Encoding:** We transform the textual categories into numerical labels using Label Encoding, allowing our machine learning algorithms to work with the data.\n",
        "- **TF-IDF Vectorization:** We convert the cleaned text data into numerical vectors using TF-IDF (Term Frequency-Inverse Document Frequency), which gives more weight to words that are specific to a document in the dataset."
      ],
      "metadata": {
        "id": "9EKrdUFpUvO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Category Encoding**"
      ],
      "metadata": {
        "id": "LUjFatQSUxTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(df['Category'])\n",
        "df['Category'] = le.transform(df['Category'])"
      ],
      "metadata": {
        "id": "ND0ORKolUyqS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Category.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AuWdUJrU3d7",
        "outputId": "7fd04ce2-a390-46d8-cf5c-b0e39a07d2f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 12,  0,  1, 24, 16, 22, 14,  5, 15,  4, 21,  2, 11, 18, 20,  8,\n",
              "       17, 19,  7, 13, 10,  9,  3, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF**"
      ],
      "metadata": {
        "id": "sLYdMl2yU67I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "tfidf.fit(df['Resume'])\n",
        "requredTaxt  = tfidf.transform(df['Resume'])"
      ],
      "metadata": {
        "id": "SvTw1x-GU_Oi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Splitting**"
      ],
      "metadata": {
        "id": "Kusy5YIRVDIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Splitting:** The dataset is divided into training and testing sets to properly evaluate the performance of our models."
      ],
      "metadata": {
        "id": "C6NTOrRxVGPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(requredTaxt, df['Category'], test_size=0.2, random_state=42)\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPBISJSfVHTi",
        "outputId": "1c11b8f8-3a90-421b-8f07-3eb7428518eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(769, 7351)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED-rghO1Vmtq",
        "outputId": "4aae2c89-77eb-46e9-a0f0-7fa97cca66cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(193, 7351)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Development**"
      ],
      "metadata": {
        "id": "wgHWtaZPVpGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Development:** We build and train multiple models:\n",
        " - **Machine Learning Models:**\n",
        "    *   **K-Nearest Neighbors (KNN):** A simple yet effective classification algorithm based on distance between points.\n",
        "    *   **Support Vector Machine (SVC):** A powerful algorithm that finds an optimal hyperplane to separate classes.\n",
        "    *   **Random Forest:** An ensemble method that combines multiple decision trees for more robust predictions.\n",
        "-   **Deep Learning Model:**\n",
        "    *   **Multilayer Perceptron (MLP):** A neural network model with multiple hidden layers to learn complex patterns from the data.\n",
        "-   **Ensemble Model**\n",
        "    *   **Voting Classifier:** Combines the predictions from the machine learning models to achieve more accurate and robust results."
      ],
      "metadata": {
        "id": "he4uP2LuVq7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Machine Learning**"
      ],
      "metadata": {
        "id": "n0LIx_DKVxXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TirlqvCZVulW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that X_train and X_test are dense if they are sparse\n",
        "X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
        "X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test"
      ],
      "metadata": {
        "id": "NnwXB778V2S6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **KNN**"
      ],
      "metadata": {
        "id": "6JHPB_B5V5e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Train KNeighborsClassifier\n",
        "knn_model = OneVsRestClassifier(KNeighborsClassifier())\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy for the training set\n",
        "y_train_pred_knn = knn_model.predict(X_train)\n",
        "train_accuracy_knn = accuracy_score(y_train, y_train_pred_knn)\n",
        "print(f\"Training Accuracy: {train_accuracy_knn:.4f}\")\n",
        "\n",
        "# Accuracy for the test set\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "test_accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(f\" Testing Accuracy: {test_accuracy_knn:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_knn)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_knn)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wep5Up4AV-FC",
        "outputId": "d1f7af28-5b7a-4071-d8cb-b7d69f759cce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9857\n",
            " Testing Accuracy: 0.9845\n",
            "Confusion Matrix:\n",
            "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       1.00      0.60      0.75         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       0.88      1.00      0.93         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       0.78      1.00      0.88         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.98       193\n",
            "   macro avg       0.99      0.98      0.98       193\n",
            "weighted avg       0.99      0.98      0.98       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Support Vector Machine**"
      ],
      "metadata": {
        "id": "ZpR615miV5b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Train SVC\n",
        "svc_model = OneVsRestClassifier(SVC())\n",
        "svc_model.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy for the training set\n",
        "y_train_pred_svc = svc_model.predict(X_train)\n",
        "train_accuracy_svc = accuracy_score(y_train, y_train_pred_svc)\n",
        "print(f\"Training Accuracy: {train_accuracy_svc:.4f}\")\n",
        "\n",
        "# Accuracy for the test set\n",
        "y_pred_svc = svc_model.predict(X_test)\n",
        "test_accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
        "print(\"\\nSVC Results:\")\n",
        "print(f\"Testing Accuracy: {test_accuracy_svc:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_svc)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_svc)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_hyy-5EWDnt",
        "outputId": "c5f8420e-df50-44bc-9d33-28702c8c30dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.0000\n",
            "\n",
            "SVC Results:\n",
            "Testing Accuracy: 0.9948\n",
            "Confusion Matrix:\n",
            "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       0.88      1.00      0.93         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.99       193\n",
            "   macro avg       0.99      1.00      1.00       193\n",
            "weighted avg       1.00      0.99      0.99       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Random Forest**"
      ],
      "metadata": {
        "id": "89zuOvMNV5ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train RandomForestClassifier\n",
        "rf_model = OneVsRestClassifier(RandomForestClassifier())\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy for the training set\n",
        "y_train_pred_rf = rf_model.predict(X_train)\n",
        "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
        "print(f\"Training Accuracy: {train_accuracy_rf:.4f}\")\n",
        "\n",
        "# Accuracy for the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"\\nRandomForestClassifier Results:\")\n",
        "print(f\"Testing Accuracy: {test_accuracy_rf:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_rf)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_rf)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDNJ4oYEWN-_",
        "outputId": "14471bc2-73a2-45e2-957c-e58f2b17d617"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.0000\n",
            "\n",
            "RandomForestClassifier Results:\n",
            "Testing Accuracy: 0.9948\n",
            "Confusion Matrix:\n",
            "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       0.88      1.00      0.93         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.99       193\n",
            "   macro avg       0.99      1.00      1.00       193\n",
            "weighted avg       1.00      0.99      0.99       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **MLP**"
      ],
      "metadata": {
        "id": "Ie1_LSidWalk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Define the MLP model\n",
        "model = keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
        "    Dense(64, activation='relu'),  # Hidden layer\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')  # Output layer (number of classes)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy for integer labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Retrieve training and validation accuracy from history\n",
        "train_accuracy = history.history['accuracy'][-1]  # Last epoch's training accuracy\n",
        "val_accuracy = history.history['val_accuracy'][-1]  # Last epoch's validation accuracy\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_mlp = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_mlp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8aolXaAWenC",
        "outputId": "4a8c6aad-2614-4742-9ad7-ae4372805661"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - accuracy: 0.2701 - loss: 3.1599 - val_accuracy: 0.4545 - val_loss: 2.8693\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6069 - loss: 2.5747 - val_accuracy: 0.6494 - val_loss: 2.0252\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8443 - loss: 1.5311 - val_accuracy: 0.9740 - val_loss: 1.0601\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9989 - loss: 0.6230 - val_accuracy: 0.9870 - val_loss: 0.4473\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.2120 - val_accuracy: 1.0000 - val_loss: 0.1959\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0864 - val_accuracy: 1.0000 - val_loss: 0.1014\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0428 - val_accuracy: 1.0000 - val_loss: 0.0696\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0258 - val_accuracy: 1.0000 - val_loss: 0.0523\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0180 - val_accuracy: 1.0000 - val_loss: 0.0429\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0151 - val_accuracy: 1.0000 - val_loss: 0.0369\n",
            "Final Training Accuracy: 1.0000\n",
            "Final Validation Accuracy: 1.0000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9973 - loss: 0.0392\n",
            "Test Loss: 0.0502\n",
            "Test Accuracy: 0.9948\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       0.83      1.00      0.91         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.99       193\n",
            "   macro avg       0.99      1.00      0.99       193\n",
            "weighted avg       1.00      0.99      0.99       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ensemble Model**"
      ],
      "metadata": {
        "id": "xg48AeyaV5Wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Voting**"
      ],
      "metadata": {
        "id": "YmAHBHYsWVuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create a VotingClassifier with the three models\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('knn', knn_model),\n",
        "    ('svc', svc_model),\n",
        "    ('rf', rf_model)\n",
        "], voting='hard')\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the ensemble model\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_ensemble)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_ensemble)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xCguBUKWTJ2",
        "outputId": "c70965e6-0b7b-4925-99a7-a03ea002e636"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model Accuracy: 0.9948\n",
            "Confusion Matrix:\n",
            "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
            "   0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       0.88      1.00      0.93         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.99       193\n",
            "   macro avg       0.99      1.00      1.00       193\n",
            "weighted avg       1.00      0.99      0.99       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deep Learning**"
      ],
      "metadata": {
        "id": "FIAjb4VsWqre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RNN**"
      ],
      "metadata": {
        "id": "y0WyXP3ybHSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "NLAuHEnqiosg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RNN with Adam**"
      ],
      "metadata": {
        "id": "oaWaHcX6ikJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['Resume'])\n",
        "sequences = tokenizer.texts_to_sequences(df['Resume'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
        "\n",
        "# Encode the labels\n",
        "le = LabelEncoder()\n",
        "le.fit(df['Category'])\n",
        "encoded_labels = le.transform(df['Category'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RNN model\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
        "    SimpleRNN(64, return_sequences=False),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "rnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_rnn = rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Retrieve training and validation accuracy from history\n",
        "train_accuracy_rnn = history_rnn.history['accuracy'][-1]\n",
        "val_accuracy_rnn = history_rnn.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy_rnn:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy_rnn:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss_rnn, test_accuracy_rnn = rnn_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss_rnn:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_rnn:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rnn = np.argmax(rnn_model.predict(X_test), axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_rnn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLOqLxXbitcE",
        "outputId": "fdef9091-ba0a-4ab9-f0d7-35f1ae0af758"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 235ms/step - accuracy: 0.3178 - loss: 3.0118 - val_accuracy: 0.7532 - val_loss: 2.5169\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8474 - loss: 2.1228 - val_accuracy: 0.8831 - val_loss: 1.6793\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9381 - loss: 1.2177 - val_accuracy: 0.8961 - val_loss: 0.9940\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9310 - loss: 0.6610 - val_accuracy: 0.9091 - val_loss: 0.6442\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9674 - loss: 0.3852 - val_accuracy: 0.9610 - val_loss: 0.3908\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9956 - loss: 0.2126 - val_accuracy: 0.9610 - val_loss: 0.2926\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9966 - loss: 0.1259 - val_accuracy: 0.9870 - val_loss: 0.2212\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9657 - loss: 0.2130 - val_accuracy: 0.9481 - val_loss: 0.3445\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9275 - loss: 0.3768 - val_accuracy: 0.9481 - val_loss: 0.4015\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9685 - loss: 0.2269 - val_accuracy: 0.9481 - val_loss: 0.2634\n",
            "Final Training Accuracy: 0.9682\n",
            "Final Validation Accuracy: 0.9481\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9420 - loss: 0.2631\n",
            "Test Loss: 0.2806\n",
            "Test Accuracy: 0.9430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bf9bc3f69e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.33      0.40         3\n",
            "           1       1.00      0.67      0.80         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      0.67      0.80         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      0.50      0.67         6\n",
            "          12       0.80      1.00      0.89        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       0.58      1.00      0.74         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       0.71      1.00      0.83         5\n",
            "\n",
            "    accuracy                           0.94       193\n",
            "   macro avg       0.94      0.92      0.92       193\n",
            "weighted avg       0.96      0.94      0.94       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RNN with GD**"
      ],
      "metadata": {
        "id": "jqvfP19ZbBvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "rnn_model.compile(optimizer=sgd_optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_rnn = rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Retrieve training and validation accuracy from history\n",
        "train_accuracy_rnn = history_rnn.history['accuracy'][-1]\n",
        "val_accuracy_rnn = history_rnn.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy_rnn:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy_rnn:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss_rnn, test_accuracy_rnn = rnn_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss_rnn:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_rnn:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rnn = np.argmax(rnn_model.predict(X_test), axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_rnn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoV_TChSirBR",
        "outputId": "5770d2b3-4a58-48b3-a674-5788c08db226"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.4104 - loss: 2.6136 - val_accuracy: 0.0390 - val_loss: 3.6470\n",
            "Epoch 2/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0684 - loss: 3.3071 - val_accuracy: 0.1429 - val_loss: 3.1616\n",
            "Epoch 3/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.0786 - loss: 3.1271 - val_accuracy: 0.0260 - val_loss: 3.2517\n",
            "Epoch 4/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.0978 - loss: 3.0579 - val_accuracy: 0.0519 - val_loss: 3.1179\n",
            "Epoch 5/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0770 - loss: 3.0604 - val_accuracy: 0.1039 - val_loss: 3.1609\n",
            "Epoch 6/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0893 - loss: 3.0538 - val_accuracy: 0.0779 - val_loss: 3.1186\n",
            "Epoch 7/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.1072 - loss: 3.0553 - val_accuracy: 0.1039 - val_loss: 3.1179\n",
            "Epoch 8/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.1095 - loss: 3.0625 - val_accuracy: 0.0519 - val_loss: 3.1601\n",
            "Epoch 9/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1179 - loss: 3.0322 - val_accuracy: 0.1039 - val_loss: 3.1166\n",
            "Epoch 10/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.0849 - loss: 3.0627 - val_accuracy: 0.0779 - val_loss: 3.0893\n",
            "Epoch 11/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1109 - loss: 2.9996 - val_accuracy: 0.0909 - val_loss: 3.0697\n",
            "Epoch 12/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0989 - loss: 3.0289 - val_accuracy: 0.1299 - val_loss: 3.0802\n",
            "Epoch 13/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1285 - loss: 3.0221 - val_accuracy: 0.1429 - val_loss: 3.0870\n",
            "Epoch 14/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.1479 - loss: 2.9651 - val_accuracy: 0.1558 - val_loss: 3.0444\n",
            "Epoch 15/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.1291 - loss: 2.9708 - val_accuracy: 0.0260 - val_loss: 3.0950\n",
            "Epoch 16/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.1279 - loss: 2.9291 - val_accuracy: 0.1299 - val_loss: 2.9924\n",
            "Epoch 17/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1612 - loss: 2.8881 - val_accuracy: 0.1429 - val_loss: 2.9641\n",
            "Epoch 18/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2394 - loss: 2.7884 - val_accuracy: 0.2338 - val_loss: 2.8352\n",
            "Epoch 19/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.1953 - loss: 2.7666 - val_accuracy: 0.2468 - val_loss: 2.7590\n",
            "Epoch 20/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.2170 - loss: 2.6713 - val_accuracy: 0.2857 - val_loss: 2.7178\n",
            "Epoch 21/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2329 - loss: 2.5706 - val_accuracy: 0.2338 - val_loss: 2.6569\n",
            "Epoch 22/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.2305 - loss: 2.4206 - val_accuracy: 0.2597 - val_loss: 2.6112\n",
            "Epoch 23/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.2919 - loss: 2.3743 - val_accuracy: 0.2468 - val_loss: 2.4927\n",
            "Epoch 24/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3012 - loss: 2.3078 - val_accuracy: 0.3247 - val_loss: 2.4462\n",
            "Epoch 25/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.3103 - loss: 2.2328 - val_accuracy: 0.2597 - val_loss: 2.4222\n",
            "Epoch 26/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3217 - loss: 2.1787 - val_accuracy: 0.2987 - val_loss: 2.3516\n",
            "Epoch 27/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.3621 - loss: 2.0612 - val_accuracy: 0.3766 - val_loss: 2.2102\n",
            "Epoch 28/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.3965 - loss: 1.9638 - val_accuracy: 0.3247 - val_loss: 2.1978\n",
            "Epoch 29/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3455 - loss: 1.9789 - val_accuracy: 0.4156 - val_loss: 2.1116\n",
            "Epoch 30/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4278 - loss: 1.8195 - val_accuracy: 0.3506 - val_loss: 2.0658\n",
            "Epoch 31/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4541 - loss: 1.8199 - val_accuracy: 0.4156 - val_loss: 2.0390\n",
            "Epoch 32/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4547 - loss: 1.7370 - val_accuracy: 0.4286 - val_loss: 1.9639\n",
            "Epoch 33/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5171 - loss: 1.6817 - val_accuracy: 0.4675 - val_loss: 1.9490\n",
            "Epoch 34/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5010 - loss: 1.7021 - val_accuracy: 0.4675 - val_loss: 1.8829\n",
            "Epoch 35/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5208 - loss: 1.6130 - val_accuracy: 0.5455 - val_loss: 1.8449\n",
            "Epoch 36/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5560 - loss: 1.5622 - val_accuracy: 0.4935 - val_loss: 1.8205\n",
            "Epoch 37/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5365 - loss: 1.5671 - val_accuracy: 0.5455 - val_loss: 1.8045\n",
            "Epoch 38/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5562 - loss: 1.5106 - val_accuracy: 0.5195 - val_loss: 1.7334\n",
            "Epoch 39/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6024 - loss: 1.3932 - val_accuracy: 0.5065 - val_loss: 1.7058\n",
            "Epoch 40/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5950 - loss: 1.4417 - val_accuracy: 0.5455 - val_loss: 1.6974\n",
            "Epoch 41/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6358 - loss: 1.4081 - val_accuracy: 0.5974 - val_loss: 1.5920\n",
            "Epoch 42/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6344 - loss: 1.3201 - val_accuracy: 0.5455 - val_loss: 1.6133\n",
            "Epoch 43/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6308 - loss: 1.3843 - val_accuracy: 0.5325 - val_loss: 1.5410\n",
            "Epoch 44/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6158 - loss: 1.3423 - val_accuracy: 0.5325 - val_loss: 1.5413\n",
            "Epoch 45/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6008 - loss: 1.3302 - val_accuracy: 0.4935 - val_loss: 1.6021\n",
            "Epoch 46/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6627 - loss: 1.2634 - val_accuracy: 0.5714 - val_loss: 1.4751\n",
            "Epoch 47/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6408 - loss: 1.2223 - val_accuracy: 0.6234 - val_loss: 1.4406\n",
            "Epoch 48/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6556 - loss: 1.2166 - val_accuracy: 0.6104 - val_loss: 1.4232\n",
            "Epoch 49/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6733 - loss: 1.1527 - val_accuracy: 0.6234 - val_loss: 1.4260\n",
            "Epoch 50/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6755 - loss: 1.1751 - val_accuracy: 0.6623 - val_loss: 1.3578\n",
            "Epoch 51/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6646 - loss: 1.1556 - val_accuracy: 0.6104 - val_loss: 1.4406\n",
            "Epoch 52/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6665 - loss: 1.1530 - val_accuracy: 0.5714 - val_loss: 1.4274\n",
            "Epoch 53/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6247 - loss: 1.2640 - val_accuracy: 0.6494 - val_loss: 1.3358\n",
            "Epoch 54/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6761 - loss: 1.1436 - val_accuracy: 0.6234 - val_loss: 1.4643\n",
            "Epoch 55/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6879 - loss: 1.0639 - val_accuracy: 0.6364 - val_loss: 1.3230\n",
            "Epoch 56/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6992 - loss: 1.0856 - val_accuracy: 0.6623 - val_loss: 1.3021\n",
            "Epoch 57/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7016 - loss: 1.0182 - val_accuracy: 0.6234 - val_loss: 1.2853\n",
            "Epoch 58/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7143 - loss: 1.0445 - val_accuracy: 0.6364 - val_loss: 1.2777\n",
            "Epoch 59/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6877 - loss: 1.0581 - val_accuracy: 0.6104 - val_loss: 1.3254\n",
            "Epoch 60/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7394 - loss: 0.8930 - val_accuracy: 0.7143 - val_loss: 1.2212\n",
            "Epoch 61/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6940 - loss: 1.0227 - val_accuracy: 0.7013 - val_loss: 1.1881\n",
            "Epoch 62/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7221 - loss: 0.9206 - val_accuracy: 0.6623 - val_loss: 1.1949\n",
            "Epoch 63/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7291 - loss: 0.9438 - val_accuracy: 0.7013 - val_loss: 1.1534\n",
            "Epoch 64/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7370 - loss: 0.9288 - val_accuracy: 0.6494 - val_loss: 1.1965\n",
            "Epoch 65/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7056 - loss: 0.9733 - val_accuracy: 0.7013 - val_loss: 1.1379\n",
            "Epoch 66/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7101 - loss: 1.0215 - val_accuracy: 0.6494 - val_loss: 1.2093\n",
            "Epoch 67/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7113 - loss: 0.9862 - val_accuracy: 0.6753 - val_loss: 1.1140\n",
            "Epoch 68/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7211 - loss: 0.9279 - val_accuracy: 0.6494 - val_loss: 1.2076\n",
            "Epoch 69/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6946 - loss: 0.9934 - val_accuracy: 0.6753 - val_loss: 1.1785\n",
            "Epoch 70/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7238 - loss: 0.9757 - val_accuracy: 0.6623 - val_loss: 1.1691\n",
            "Epoch 71/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7178 - loss: 0.9801 - val_accuracy: 0.6623 - val_loss: 1.1076\n",
            "Epoch 72/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7424 - loss: 0.8403 - val_accuracy: 0.6883 - val_loss: 1.1167\n",
            "Epoch 73/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7536 - loss: 0.8339 - val_accuracy: 0.6753 - val_loss: 1.1086\n",
            "Epoch 74/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7181 - loss: 0.8979 - val_accuracy: 0.6753 - val_loss: 1.0905\n",
            "Epoch 75/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7251 - loss: 0.8921 - val_accuracy: 0.6883 - val_loss: 1.0590\n",
            "Epoch 76/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7208 - loss: 0.9206 - val_accuracy: 0.6753 - val_loss: 1.0716\n",
            "Epoch 77/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7428 - loss: 0.8617 - val_accuracy: 0.6883 - val_loss: 1.0684\n",
            "Epoch 78/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7392 - loss: 0.8815 - val_accuracy: 0.7013 - val_loss: 1.0537\n",
            "Epoch 79/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7470 - loss: 0.8896 - val_accuracy: 0.7013 - val_loss: 1.0701\n",
            "Epoch 80/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7483 - loss: 0.8129 - val_accuracy: 0.7143 - val_loss: 1.0211\n",
            "Epoch 81/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6941 - loss: 0.9573 - val_accuracy: 0.7013 - val_loss: 1.0244\n",
            "Epoch 82/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7194 - loss: 0.9165 - val_accuracy: 0.6623 - val_loss: 1.1059\n",
            "Epoch 83/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7232 - loss: 0.9485 - val_accuracy: 0.6883 - val_loss: 1.0356\n",
            "Epoch 84/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7387 - loss: 0.8732 - val_accuracy: 0.6753 - val_loss: 1.0986\n",
            "Epoch 85/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7372 - loss: 0.8808 - val_accuracy: 0.7013 - val_loss: 1.0418\n",
            "Epoch 86/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7320 - loss: 0.8838 - val_accuracy: 0.6753 - val_loss: 1.0245\n",
            "Epoch 87/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7097 - loss: 0.9002 - val_accuracy: 0.7013 - val_loss: 0.9809\n",
            "Epoch 88/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7330 - loss: 0.8311 - val_accuracy: 0.6623 - val_loss: 1.0481\n",
            "Epoch 89/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7369 - loss: 0.8559 - val_accuracy: 0.6883 - val_loss: 1.0054\n",
            "Epoch 90/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7588 - loss: 0.7923 - val_accuracy: 0.6883 - val_loss: 1.0681\n",
            "Epoch 91/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7292 - loss: 0.8341 - val_accuracy: 0.7143 - val_loss: 1.0171\n",
            "Epoch 92/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7532 - loss: 0.8191 - val_accuracy: 0.6623 - val_loss: 1.0356\n",
            "Epoch 93/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7042 - loss: 0.9352 - val_accuracy: 0.7143 - val_loss: 1.0113\n",
            "Epoch 94/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7608 - loss: 0.7865 - val_accuracy: 0.7273 - val_loss: 0.9927\n",
            "Epoch 95/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7315 - loss: 0.8513 - val_accuracy: 0.6623 - val_loss: 1.0075\n",
            "Epoch 96/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7489 - loss: 0.8161 - val_accuracy: 0.6753 - val_loss: 1.0285\n",
            "Epoch 97/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7463 - loss: 0.7917 - val_accuracy: 0.7143 - val_loss: 1.0645\n",
            "Epoch 98/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7474 - loss: 0.8344 - val_accuracy: 0.6883 - val_loss: 1.0250\n",
            "Epoch 99/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7207 - loss: 0.8559 - val_accuracy: 0.6883 - val_loss: 1.0005\n",
            "Epoch 100/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7381 - loss: 0.8411 - val_accuracy: 0.7143 - val_loss: 1.1506\n",
            "Final Training Accuracy: 0.7283\n",
            "Final Validation Accuracy: 0.7143\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7078 - loss: 1.0599\n",
            "Test Loss: 1.1399\n",
            "Test Accuracy: 0.7047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bfa295f91b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.33      0.50         3\n",
            "           1       0.67      0.33      0.44         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      0.56      0.71         9\n",
            "           6       1.00      0.80      0.89         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      0.60      0.75         5\n",
            "          10       1.00      0.71      0.83         7\n",
            "          11       1.00      0.50      0.67         6\n",
            "          12       0.00      0.00      0.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      0.43      0.60         7\n",
            "          15       0.21      1.00      0.35        15\n",
            "          16       1.00      0.62      0.77         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      0.50      0.67        10\n",
            "          21       1.00      0.86      0.92         7\n",
            "          22       1.00      0.38      0.55         8\n",
            "          23       1.00      0.62      0.77        16\n",
            "          24       1.00      0.60      0.75         5\n",
            "\n",
            "    accuracy                           0.70       193\n",
            "   macro avg       0.92      0.71      0.77       193\n",
            "weighted avg       0.87      0.70      0.73       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LSTM**"
      ],
      "metadata": {
        "id": "70QY5LgXWvzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['Resume'])\n",
        "sequences = tokenizer.texts_to_sequences(df['Resume'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
        "\n",
        "# Encode the labels\n",
        "le = LabelEncoder()\n",
        "le.fit(df['Category'])\n",
        "encoded_labels = le.transform(df['Category'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    GlobalMaxPool1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Retrieve training and validation accuracy from history\n",
        "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
        "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lstm = np.argmax(lstm_model.predict(X_test), axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG4bw9D0WrnU",
        "outputId": "7f06caf6-ef74-4727-cb2b-e8cd24c6d543"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.0814 - loss: 3.1957 - val_accuracy: 0.2468 - val_loss: 3.0749\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2456 - loss: 2.9919 - val_accuracy: 0.2468 - val_loss: 2.8769\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2296 - loss: 2.7831 - val_accuracy: 0.2857 - val_loss: 2.5922\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.3308 - loss: 2.3399 - val_accuracy: 0.4545 - val_loss: 2.2734\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5506 - loss: 1.9703 - val_accuracy: 0.5325 - val_loss: 1.8394\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6855 - loss: 1.4866 - val_accuracy: 0.7143 - val_loss: 1.4066\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8313 - loss: 1.0664 - val_accuracy: 0.6883 - val_loss: 1.2488\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9101 - loss: 0.6638 - val_accuracy: 0.8831 - val_loss: 0.6887\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9411 - loss: 0.4761 - val_accuracy: 0.9481 - val_loss: 0.4432\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9874 - loss: 0.2609 - val_accuracy: 0.9740 - val_loss: 0.3152\n",
            "Final Training Accuracy: 0.9841\n",
            "Final Validation Accuracy: 0.9740\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9720 - loss: 0.2736 \n",
            "Test Loss: 0.2816\n",
            "Test Accuracy: 0.9741\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      0.44      0.62         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       0.80      1.00      0.89         8\n",
            "           8       1.00      1.00      1.00        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.97       193\n",
            "   macro avg       0.97      0.98      0.97       193\n",
            "weighted avg       0.98      0.97      0.97       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LSTM with GD**"
      ],
      "metadata": {
        "id": "Y-3NOG1TYwb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Compile the model with SGD optimizer\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "lstm_model.compile(optimizer=sgd_optimizer,\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Retrieve training and validation accuracy from history\n",
        "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
        "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lstm = np.argmax(lstm_model.predict(X_test), axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxnE1EBsYxHH",
        "outputId": "4b0adc8d-4465-4e6f-aea9-98b8471a27d9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9809 - loss: 0.2508 - val_accuracy: 0.8831 - val_loss: 0.8441\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9100 - loss: 0.6410 - val_accuracy: 0.9091 - val_loss: 0.6245\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9758 - loss: 0.3674 - val_accuracy: 0.9740 - val_loss: 0.3776\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9898 - loss: 0.2382 - val_accuracy: 0.9740 - val_loss: 0.2624\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9931 - loss: 0.1665 - val_accuracy: 0.9740 - val_loss: 0.2051\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9970 - loss: 0.1216 - val_accuracy: 0.9740 - val_loss: 0.1826\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9959 - loss: 0.1054 - val_accuracy: 0.9870 - val_loss: 0.1475\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0800 - val_accuracy: 0.9610 - val_loss: 0.1447\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0693 - val_accuracy: 0.9870 - val_loss: 0.1126\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0557 - val_accuracy: 0.9870 - val_loss: 0.1098\n",
            "Final Training Accuracy: 1.0000\n",
            "Final Validation Accuracy: 0.9870\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9871 - loss: 0.0919 \n",
            "Test Loss: 0.1064\n",
            "Test Accuracy: 0.9793\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       0.86      0.67      0.75         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.98       193\n",
            "   macro avg       0.97      0.98      0.98       193\n",
            "weighted avg       0.99      0.98      0.98       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BI-LSTM**"
      ],
      "metadata": {
        "id": "RVZuKrD3ZVgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['Resume'])\n",
        "sequences = tokenizer.texts_to_sequences(df['Resume'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
        "\n",
        "# Encode the labels\n",
        "le = LabelEncoder()\n",
        "le.fit(df['Category'])\n",
        "encoded_labels = le.transform(df['Category'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the BI-LSTM model\n",
        "bi_lstm_model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    GlobalMaxPool1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "bi_lstm_model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_bi_lstm = bi_lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Retrieve training and validation accuracy from history\n",
        "train_accuracy_bi_lstm = history_bi_lstm.history['accuracy'][-1]\n",
        "val_accuracy_bi_lstm = history_bi_lstm.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy_bi_lstm:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy_bi_lstm:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss_bi_lstm, test_accuracy_bi_lstm = bi_lstm_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss_bi_lstm:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_bi_lstm:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_bi_lstm = np.argmax(bi_lstm_model.predict(X_test), axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_bi_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoZGpYoHZZLB",
        "outputId": "63ffa228-c25b-4fb2-8236-0f1530d773a4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 66ms/step - accuracy: 0.1615 - loss: 3.1908 - val_accuracy: 0.1948 - val_loss: 3.0863\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3121 - loss: 2.9855 - val_accuracy: 0.2078 - val_loss: 2.7326\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.3492 - loss: 2.4300 - val_accuracy: 0.3766 - val_loss: 2.0412\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6359 - loss: 1.6027 - val_accuracy: 0.7403 - val_loss: 1.2891\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.8909 - loss: 0.8487 - val_accuracy: 0.8701 - val_loss: 0.7302\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9691 - loss: 0.4047 - val_accuracy: 0.9221 - val_loss: 0.3561\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9853 - loss: 0.1919 - val_accuracy: 0.9870 - val_loss: 0.1822\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9956 - loss: 0.1166 - val_accuracy: 1.0000 - val_loss: 0.1357\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9997 - loss: 0.0717 - val_accuracy: 1.0000 - val_loss: 0.0817\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0455 - val_accuracy: 1.0000 - val_loss: 0.0483\n",
            "Final Training Accuracy: 1.0000\n",
            "Final Validation Accuracy: 1.0000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9814 - loss: 0.0572\n",
            "Test Loss: 0.0604\n",
            "Test Accuracy: 0.9845\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.33      0.50         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      0.93      0.96        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       0.86      1.00      0.92         6\n",
            "          12       0.86      1.00      0.92        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.98       193\n",
            "   macro avg       0.99      0.97      0.97       193\n",
            "weighted avg       0.99      0.98      0.98       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BI-LSTM with GD**\n"
      ],
      "metadata": {
        "id": "I369GuThZicz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Compile the model with SGD optimizer\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "bi_lstm_model.compile(optimizer=sgd_optimizer,\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_bi_lstm = bi_lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Retrieve training and validation accuracy from history\n",
        "train_accuracy_bi_lstm = history_bi_lstm.history['accuracy'][-1]\n",
        "val_accuracy_bi_lstm = history_bi_lstm.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Accuracy: {train_accuracy_bi_lstm:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy_bi_lstm:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss_bi_lstm, test_accuracy_bi_lstm = bi_lstm_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss_bi_lstm:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_bi_lstm:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_bi_lstm = np.argmax(bi_lstm_model.predict(X_test), axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_bi_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mZYMXBsZs6j",
        "outputId": "a5ff34b2-5dcc-4f06-fa6e-70edfc38ee3a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0277 - val_accuracy: 1.0000 - val_loss: 0.0402\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0246 - val_accuracy: 1.0000 - val_loss: 0.0349\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0228 - val_accuracy: 1.0000 - val_loss: 0.0322\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0190 - val_accuracy: 1.0000 - val_loss: 0.0287\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0161 - val_accuracy: 1.0000 - val_loss: 0.0267\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0163 - val_accuracy: 1.0000 - val_loss: 0.0240\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 0.0224\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0130 - val_accuracy: 1.0000 - val_loss: 0.0211\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0131 - val_accuracy: 1.0000 - val_loss: 0.0196\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 1.0000 - val_loss: 0.0186\n",
            "Final Training Accuracy: 1.0000\n",
            "Final Validation Accuracy: 1.0000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0254 \n",
            "Test Loss: 0.0252\n",
            "Test Accuracy: 1.0000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      1.00      1.00         6\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         7\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         8\n",
            "           8       1.00      1.00      1.00        14\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         7\n",
            "          11       1.00      1.00      1.00         6\n",
            "          12       1.00      1.00      1.00        12\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        15\n",
            "          16       1.00      1.00      1.00         8\n",
            "          17       1.00      1.00      1.00         3\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      1.00      1.00        10\n",
            "          21       1.00      1.00      1.00         7\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        16\n",
            "          24       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           1.00       193\n",
            "   macro avg       1.00      1.00      1.00       193\n",
            "weighted avg       1.00      1.00      1.00       193\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT language model**"
      ],
      "metadata": {
        "id": "gk9vMSWuTlGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "I_J9D_GGHxFs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/UpdatedResumeDataSet.csv\")\n",
        "df.head()\n",
        "\n",
        "# Preprocess Text\n",
        "df['Resume'] = df['Resume'].apply(cleanResume)\n",
        "\n",
        "# Encode labels to numerical\n",
        "label_encoder = LabelEncoder()\n",
        "df['Category'] = label_encoder.fit_transform(df['Category'])\n",
        "\n",
        "# Split into train and validation\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HGMCXKpxH1xL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResumeDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.dataframe.iloc[idx]['Resume'])\n",
        "        label = int(self.dataframe.iloc[idx]['Category'])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "uL9lDlDxH4Ez"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a BERT model variant (e.g., 'bert-base-uncased', 'bert-large-uncased')\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Find the number of labels\n",
        "num_labels= len(label_encoder.classes_)\n",
        "\n",
        "# Initialize the BERT model for classification\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Maximum sequence length\n",
        "max_length = 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVNAmlKSIUkX",
        "outputId": "37ee1c6c-7d81-4002-8b57-a6c471778af1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom datasets\n",
        "train_dataset = ResumeDataset(train_df, tokenizer, max_length)\n",
        "val_dataset = ResumeDataset(val_df, tokenizer, max_length)\n",
        "\n",
        "# Create dataloaders for train and validation dataset\n",
        "batch_size = 8\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Wts9dP7GIZ5v"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# AdamW optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "cCEKuSVRIej8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a variable to store the final validation accuracy\n",
        "final_val_accuracy = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_true_labels = []\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        train_predictions.extend(predictions.cpu().numpy())\n",
        "        train_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss}, Training Accuracy: {train_accuracy}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            val_predictions.extend(predictions.cpu().numpy())\n",
        "            val_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "    final_val_accuracy = accuracy  # Update the final accuracy after each epoch\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Testing Loss: {avg_val_loss}, Testing Accuracy: {accuracy}\")\n",
        "    print(classification_report(val_true_labels, val_predictions, target_names=label_encoder.classes_))\n",
        "\n",
        "# Print the final validation accuracy after all epochs\n",
        "print(f\"Final Validation Accuracy: {final_val_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7n-_4elIgvh",
        "outputId": "418ef741-c064-40c3-9d6b-6cafc26a841c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Training Loss: 2.997423695534775, Training Accuracy: 0.1625487646293888\n",
            "Epoch 1/3, Testing Loss: 2.4730951166152955, Testing Accuracy: 0.37823834196891193\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "                 Advocate       1.00      0.33      0.50         3\n",
            "                     Arts       1.00      0.17      0.29         6\n",
            "       Automation Testing       0.00      0.00      0.00         5\n",
            "               Blockchain       0.00      0.00      0.00         7\n",
            "         Business Analyst       0.14      0.25      0.18         4\n",
            "           Civil Engineer       0.00      0.00      0.00         9\n",
            "             Data Science       0.14      1.00      0.25         5\n",
            "                 Database       0.00      0.00      0.00         8\n",
            "          DevOps Engineer       0.00      0.00      0.00        14\n",
            "         DotNet Developer       0.00      0.00      0.00         5\n",
            "            ETL Developer       0.00      0.00      0.00         7\n",
            "   Electrical Engineering       0.00      0.00      0.00         6\n",
            "                       HR       0.69      0.75      0.72        12\n",
            "                   Hadoop       0.14      1.00      0.24         4\n",
            "       Health and fitness       0.00      0.00      0.00         7\n",
            "           Java Developer       0.50      1.00      0.67        15\n",
            "      Mechanical Engineer       0.00      0.00      0.00         8\n",
            "Network Security Engineer       0.00      0.00      0.00         3\n",
            "       Operations Manager       1.00      0.75      0.86        12\n",
            "                      PMO       0.00      0.00      0.00         7\n",
            "         Python Developer       1.00      0.30      0.46        10\n",
            "            SAP Developer       0.00      0.00      0.00         7\n",
            "                    Sales       0.30      1.00      0.46         8\n",
            "                  Testing       0.43      1.00      0.60        16\n",
            "            Web Designing       1.00      0.20      0.33         5\n",
            "\n",
            "                 accuracy                           0.38       193\n",
            "                macro avg       0.29      0.31      0.22       193\n",
            "             weighted avg       0.33      0.38      0.28       193\n",
            "\n",
            "Epoch 2/3, Training Loss: 1.974184556105702, Training Accuracy: 0.7009102730819246\n",
            "Epoch 2/3, Testing Loss: 1.3623307156562805, Testing Accuracy: 0.9119170984455959\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "                 Advocate       1.00      1.00      1.00         3\n",
            "                     Arts       1.00      1.00      1.00         6\n",
            "       Automation Testing       1.00      1.00      1.00         5\n",
            "               Blockchain       1.00      1.00      1.00         7\n",
            "         Business Analyst       1.00      1.00      1.00         4\n",
            "           Civil Engineer       1.00      1.00      1.00         9\n",
            "             Data Science       0.50      1.00      0.67         5\n",
            "                 Database       1.00      0.12      0.22         8\n",
            "          DevOps Engineer       1.00      0.93      0.96        14\n",
            "         DotNet Developer       1.00      0.20      0.33         5\n",
            "            ETL Developer       0.39      1.00      0.56         7\n",
            "   Electrical Engineering       1.00      1.00      1.00         6\n",
            "                       HR       1.00      1.00      1.00        12\n",
            "                   Hadoop       1.00      1.00      1.00         4\n",
            "       Health and fitness       1.00      1.00      1.00         7\n",
            "           Java Developer       1.00      1.00      1.00        15\n",
            "      Mechanical Engineer       1.00      1.00      1.00         8\n",
            "Network Security Engineer       1.00      1.00      1.00         3\n",
            "       Operations Manager       1.00      1.00      1.00        12\n",
            "                      PMO       0.88      1.00      0.93         7\n",
            "         Python Developer       1.00      1.00      1.00        10\n",
            "            SAP Developer       1.00      0.29      0.44         7\n",
            "                    Sales       1.00      1.00      1.00         8\n",
            "                  Testing       1.00      1.00      1.00        16\n",
            "            Web Designing       1.00      1.00      1.00         5\n",
            "\n",
            "                 accuracy                           0.91       193\n",
            "                macro avg       0.95      0.90      0.88       193\n",
            "             weighted avg       0.96      0.91      0.90       193\n",
            "\n",
            "Epoch 3/3, Training Loss: 1.0177161079092123, Training Accuracy: 0.9687906371911573\n",
            "Epoch 3/3, Testing Loss: 0.6144106423854828, Testing Accuracy: 0.9948186528497409\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "                 Advocate       1.00      1.00      1.00         3\n",
            "                     Arts       1.00      1.00      1.00         6\n",
            "       Automation Testing       1.00      1.00      1.00         5\n",
            "               Blockchain       1.00      1.00      1.00         7\n",
            "         Business Analyst       1.00      1.00      1.00         4\n",
            "           Civil Engineer       1.00      1.00      1.00         9\n",
            "             Data Science       0.83      1.00      0.91         5\n",
            "                 Database       1.00      1.00      1.00         8\n",
            "          DevOps Engineer       1.00      0.93      0.96        14\n",
            "         DotNet Developer       1.00      1.00      1.00         5\n",
            "            ETL Developer       1.00      1.00      1.00         7\n",
            "   Electrical Engineering       1.00      1.00      1.00         6\n",
            "                       HR       1.00      1.00      1.00        12\n",
            "                   Hadoop       1.00      1.00      1.00         4\n",
            "       Health and fitness       1.00      1.00      1.00         7\n",
            "           Java Developer       1.00      1.00      1.00        15\n",
            "      Mechanical Engineer       1.00      1.00      1.00         8\n",
            "Network Security Engineer       1.00      1.00      1.00         3\n",
            "       Operations Manager       1.00      1.00      1.00        12\n",
            "                      PMO       1.00      1.00      1.00         7\n",
            "         Python Developer       1.00      1.00      1.00        10\n",
            "            SAP Developer       1.00      1.00      1.00         7\n",
            "                    Sales       1.00      1.00      1.00         8\n",
            "                  Testing       1.00      1.00      1.00        16\n",
            "            Web Designing       1.00      1.00      1.00         5\n",
            "\n",
            "                 accuracy                           0.99       193\n",
            "                macro avg       0.99      1.00      0.99       193\n",
            "             weighted avg       1.00      0.99      0.99       193\n",
            "\n",
            "Final Validation Accuracy: 0.9948186528497409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(resume_text, tokenizer, model, max_length, device, label_encoder):\n",
        "    model.eval()\n",
        "    cleaned_text = cleanResume(resume_text)\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        cleaned_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "        )\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, attention_mask=attention_mask)\n",
        "        predicted_class = torch.argmax(output.logits, dim=-1).cpu().item()\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
        "    return predicted_label"
      ],
      "metadata": {
        "id": "xptLN60iIo8S"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "sample_resume = \"\"\"Name: Sarah Johnson\n",
        "Contact Information:\n",
        "\n",
        "Phone: +1 555-123-4567\n",
        "Email: sarah.johnson@email.com\n",
        "LinkedIn: linkedin.com/in/sarahjohnson\n",
        "Address: New York, NY\n",
        "Professional Summary\n",
        "A results-driven HR professional with over 6 years of experience in talent acquisition, employee engagement, and HR operations. Skilled in building strong teams, fostering positive work environments, and implementing HR strategies that align with organizational goals. Proficient in HR software and data-driven decision-making to improve workforce management.\n",
        "\n",
        "Key Skills\n",
        "Talent Acquisition and Recruitment\n",
        "Employee Onboarding and Training\n",
        "Performance Management\n",
        "HR Policies and Compliance\n",
        "Employee Relations and Engagement\n",
        "Compensation and Benefits Administration\n",
        "HR Analytics and Reporting\n",
        "HR Software: SAP, Workday, BambooHR\n",
        "Strong Interpersonal and Communication Skills\n",
        "Professional Experience\n",
        "HR Manager\n",
        "BrightPath Solutions | January 2020 – Present\n",
        "\n",
        "Led end-to-end recruitment processes, successfully hiring over 50 candidates annually across various roles.\n",
        "Designed and implemented onboarding programs, reducing new hire turnover by 20%.\n",
        "Developed performance management frameworks, increasing employee productivity by 15%.\n",
        "Conducted employee satisfaction surveys and implemented strategies to enhance engagement.\n",
        "Ensured compliance with labor laws and company policies, minimizing legal risks.\n",
        "HR Generalist\n",
        "Global Reach Inc. | June 2016 – December 2019\n",
        "\n",
        "Supported HR operations, including recruitment, payroll, and benefits administration.\n",
        "Assisted in developing HR policies and communicated updates to employees.\n",
        "Resolved employee grievances, fostering a collaborative workplace.\n",
        "Analyzed HR metrics to identify trends and presented actionable insights to management.\n",
        "HR Coordinator\n",
        "TalentFirst Consulting | March 2014 – May 2016\n",
        "\n",
        "Scheduled interviews and coordinated recruitment activities.\n",
        "Maintained employee records and ensured accuracy in HR databases.\n",
        "Assisted in planning company events and training sessions.\n",
        "Education\n",
        "Bachelor’s Degree in Human Resource Management\n",
        "University of California, Berkeley | 2013\n",
        "\n",
        "Certifications\n",
        "\n",
        "Certified Professional in Human Resources (PHR)\n",
        "SHRM Certified Professional (SHRM-CP)\n",
        "Advanced HR Analytics (Coursera)\n",
        "Achievements\n",
        "Reduced time-to-hire by 30% through process optimization.\n",
        "Increased employee retention by 25% by implementing a mentorship program.\n",
        "Spearheaded diversity and inclusion initiatives, leading to a 40% increase in diverse hires.\n",
        "Languages\n",
        "English (Fluent)\n",
        "Spanish (Intermediate)\n",
        "\"\"\"\n",
        "\n",
        "predicted_category = predict_category(sample_resume, tokenizer, model, max_length, device, label_encoder)\n",
        "print(f\"Predicted Category: {predicted_category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfwd4dXjLbZi",
        "outputId": "ac20d72b-6a7e-4988-feaa-f32a9cba9180"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Category: HR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3TXh1AbQ5gF"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVtTJa4SQ5gF"
      },
      "source": [
        "**Prediction:** The app provides predictions from all the trained models, allowing for an extensive comparison of their results. The user receives a result from each model, including KNN, SVC, Random Forest, MLP, and the ensemble model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Dh5rQVWlQ5gF"
      },
      "outputs": [],
      "source": [
        "# Function to predict the category of a resume and print results for each model\n",
        "def pred(input_resume):\n",
        "    # Preprocess the input text (e.g., cleaning, etc.)\n",
        "    cleaned_text = cleanResume(input_resume)\n",
        "\n",
        "    # Vectorize the cleaned text using the same TF-IDF vectorizer used during training\n",
        "    vectorized_text = tfidf.transform([cleaned_text])\n",
        "\n",
        "    # Convert sparse matrix to dense\n",
        "    vectorized_text = vectorized_text.toarray()\n",
        "\n",
        "    # Prediction\n",
        "    predicted_category_knn = knn_model.predict(vectorized_text)\n",
        "    predicted_category_svc = svc_model.predict(vectorized_text)\n",
        "    predicted_category_rf = rf_model.predict(vectorized_text)\n",
        "    predicted_category_mlp = np.argmax(model.predict(vectorized_text), axis=1)\n",
        "    predicted_category_ensemble = ensemble_model.predict(vectorized_text)\n",
        "\n",
        "    # Get name of predicted category for each model\n",
        "    category_knn = le.inverse_transform(predicted_category_knn)[0]\n",
        "    category_svc = le.inverse_transform(predicted_category_svc)[0]\n",
        "    category_rf = le.inverse_transform(predicted_category_rf)[0]\n",
        "    category_mlp = le.inverse_transform(predicted_category_mlp)[0]\n",
        "    category_ensemble = le.inverse_transform(predicted_category_ensemble)[0]\n",
        "\n",
        "    # Print results for each model\n",
        "    print(f\"KNN Model Prediction: {category_knn}\")\n",
        "    print(f\"SVC Model Prediction: {category_svc}\")\n",
        "    print(f\"Random Forest Model Prediction: {category_rf}\")\n",
        "    print(f\"MLP Model Prediction: {category_mlp}\")\n",
        "    print(f\"Ensemble Model Prediction: {category_ensemble}\")\n",
        "\n",
        "    # Return the category name predicted by the first model (as an example)\n",
        "    return category_knn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "85CrEPFhQ5gF",
        "outputId": "db3bd337-7ac9-4f84-d374-8929080d5157"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'BertForSequenceClassification' object has no attribute 'predict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-a12ac25f237f>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \"\"\"\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-3a805d760585>\u001b[0m in \u001b[0;36mpred\u001b[0;34m(input_resume)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpredicted_category_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpredicted_category_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpredicted_category_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mpredicted_category_ensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "myresume = \"\"\"Name: Sarah Johnson\n",
        "Contact Information:\n",
        "\n",
        "Phone: +1 555-123-4567\n",
        "Email: sarah.johnson@email.com\n",
        "LinkedIn: linkedin.com/in/sarahjohnson\n",
        "Address: New York, NY\n",
        "Professional Summary\n",
        "A results-driven HR professional with over 6 years of experience in talent acquisition, employee engagement, and HR operations. Skilled in building strong teams, fostering positive work environments, and implementing HR strategies that align with organizational goals. Proficient in HR software and data-driven decision-making to improve workforce management.\n",
        "\n",
        "Key Skills\n",
        "Talent Acquisition and Recruitment\n",
        "Employee Onboarding and Training\n",
        "Performance Management\n",
        "HR Policies and Compliance\n",
        "Employee Relations and Engagement\n",
        "Compensation and Benefits Administration\n",
        "HR Analytics and Reporting\n",
        "HR Software: SAP, Workday, BambooHR\n",
        "Strong Interpersonal and Communication Skills\n",
        "Professional Experience\n",
        "HR Manager\n",
        "BrightPath Solutions | January 2020 – Present\n",
        "\n",
        "Led end-to-end recruitment processes, successfully hiring over 50 candidates annually across various roles.\n",
        "Designed and implemented onboarding programs, reducing new hire turnover by 20%.\n",
        "Developed performance management frameworks, increasing employee productivity by 15%.\n",
        "Conducted employee satisfaction surveys and implemented strategies to enhance engagement.\n",
        "Ensured compliance with labor laws and company policies, minimizing legal risks.\n",
        "HR Generalist\n",
        "Global Reach Inc. | June 2016 – December 2019\n",
        "\n",
        "Supported HR operations, including recruitment, payroll, and benefits administration.\n",
        "Assisted in developing HR policies and communicated updates to employees.\n",
        "Resolved employee grievances, fostering a collaborative workplace.\n",
        "Analyzed HR metrics to identify trends and presented actionable insights to management.\n",
        "HR Coordinator\n",
        "TalentFirst Consulting | March 2014 – May 2016\n",
        "\n",
        "Scheduled interviews and coordinated recruitment activities.\n",
        "Maintained employee records and ensured accuracy in HR databases.\n",
        "Assisted in planning company events and training sessions.\n",
        "Education\n",
        "Bachelor’s Degree in Human Resource Management\n",
        "University of California, Berkeley | 2013\n",
        "\n",
        "Certifications\n",
        "\n",
        "Certified Professional in Human Resources (PHR)\n",
        "SHRM Certified Professional (SHRM-CP)\n",
        "Advanced HR Analytics (Coursera)\n",
        "Achievements\n",
        "Reduced time-to-hire by 30% through process optimization.\n",
        "Increased employee retention by 25% by implementing a mentorship program.\n",
        "Spearheaded diversity and inclusion initiatives, leading to a 40% increase in diverse hires.\n",
        "Languages\n",
        "English (Fluent)\n",
        "Spanish (Intermediate)\n",
        "\"\"\"\n",
        "\n",
        "pred(myresume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDxIlwiEQ5gF"
      },
      "outputs": [],
      "source": [
        "myresume = \"\"\"I am a Business Analyst specializing in developing dashboards,\n",
        "reports, and data models to drive performance insights.\n",
        "Proficient in Python, R, SQL, Excel, and Power BI, I excel in data\n",
        "analysis, advanced analytics, and automation of data processes.\n",
        "Skilled in statistical analysis and data visualization, I derive\n",
        "insights for data-driven decisions. Experienced in designing and\n",
        "optimizing data warehouse solutions, managing ETL processes,\n",
        "and ensuring data integrity and security. Additionally, I hold a\n",
        "CCNA certification from Cisco, showcasing my knowledge in\n",
        "networking.\n",
        "\"\"\"\n",
        "\n",
        "pred(myresume)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}